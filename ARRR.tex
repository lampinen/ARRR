\documentclass[11pt]{article}
\usepackage[fleqn]{amsmath}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{mathrsfs}
\usepackage{graphicx}
\usepackage{float}
\usepackage{subcaption}
\usepackage{relsize}
\usepackage{listings}
\usepackage{tikz}
\usepackage{natbib}
%\usepackage{breqn}
\lstset{language=Python,
    frame=single,
    breaklines=true,
    postbreak=\raisebox{0ex}[0ex][0ex]{\ensuremath{\color{red}\hookrightarrow\space}}
}
%%\usepackage[margin=2cm]{geometry}
%%\floatstyle{boxed}
\restylefloat{figure}
\newcommand{\Prop}{\textbf{Proposition: }}
\newcommand{\Prob}{\textbf{Problem: }}
\newcommand{\Prf}{\textbf{Proof: }}
\newcommand{\Sol}{\textbf{Solution: }}
\newcommand{\grad}{\nabla}
\newcommand{\Nats}{\mathbb{N}}
\newcommand{\Ints}{\mathbb{Z}}
\newcommand{\Rats}{\mathbb{Q}}
\newcommand{\Reals}{\mathbb{R}}
\newcommand{\Comps}{\mathbb{C}}
\newcommand{\Prb}[1]{P\left( #1 \right)}
\newcommand{\PT}[1]{P\left( \text{#1} \right)}
\newcommand{\PCon}[2]{P\left( #1 \mid #2 \right)}
\newcommand{\PConT}[2]{P\left( \text{#1} \mid \text{#2} \right)}
\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator{\tr}{\textbf{tr}}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}
\newcommand{\thus}{\quad\mathlarger{\mathlarger{\mathlarger{\Rightarrow}}}\quad} 
\newcommand{\wght}{\mathbf{w}}
\newcommand{\im}{\text{im }}
\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm,nohead]{geometry}
%%\newcommand\scalemath[2]{\scalebox{#1}{\mbox{\ensuremath{\displaystyle #2}}}}
\setlength\parindent{0pt}
\parskip = \baselineskip
\begin{document} 
\title{ARRR: Transfer and flexibility}
\author{Andrew Lampinen}
\date{}
\maketitle
\section{Introduction}
Deep learning methods have achieved incredible success recently, achieving human level performance in domains ranging from vision \citep[e.g.]{Szegedy2015} to playing games \citep[e.g.]{Silver2016}. Yet these systems lack some important human abilities \citep[e.g.]{Lake2016}. Deep learning models are data-hungry, while humans can frequently learn from relatively few examples. \par 
Furthermore, even if they are given a large amount of data, deep learning systems may not be able to generalize well outside the data distribution they trained on. Once knowledge is learned in the weights of a deep learning model, it is difficult to flexibly reuse that knowledge. \par
In particular, humans can flexibly remap their behavior in a way completely inconsistent with their prior behavior on a task, such as trying to lose a game they have previously been trying to win \citep{Lake2016}. More generally, humans are often able to accurately behave according to linguistic instructions without seeing examples of such behavior. This type of ``zero-shot'' learning can be quite difficult for deep learning systems. \par 
Some of these problems have been partially addressed in the machine learning literature. Two of the most important ideas in this context are multi-task learning and meta-learning, which have been shown to help with some of these issues. However, there are many human-like abilities that remain frustratingly out of reach with contemporary methods. I will first review some of the cognitive issues and the progress to date, try to provide a unifying perspective on how transfer and meta-learning contribute to human learning, and show some new research and future directions on this topic. \par

\section{Human flexibility}

What kind of flexibility do humans have? We are often able to learn rapidly. For example, we can achieve some competence in a video game we've never seen before within a few minutes \citep{Lake2016}. We can learn new concepts from seeing only a relatively small number of examples \citep[e.g.]{Bourne1970}. We can often learn even faster if we can actively participate in the learning process by selecting examples rather than passively receiving them, especially if the concepts are simple enough that we can generate a good hypothesis space \citep{Markant2014a}. \par
We can then apply what we have learned to new situations. The studies of \citet{Bourne1970} that I referenced above show that once people have learned a new concept from examples, they can generalize that knowedge to learn structurally-similar new concepts more rapidly. Indeed, even without explicit awareness of the relationship between two tasks, humans can sometimes benefit from transfer effects \citep[e.g.]{Day2011}. In general, human analogical transfer abilities have been suggested to be a critical component of `` what makes us smart'' \citep{Gentner2003}. \par
Yet human flexibility is apparent beyond transfer between isomorphic tasks. We can often competently change our learned behavior in response to instruction or other goals, such as trying to lose a game we were previously trying to win or achieve some orthogonal task \citep{Lake2016}. Indeed, it has been known for almost a century that even other animals exhibit this sort of flexible knowledge use -- they engage in ``latent learning'' of environmental features that may be useful when solving future tasks \citep{Blodgett1929}. Humans are capable of reusing our knowledge flexibly in many situations. \par
However, human flexibilty is not universal. Sometimes it is quite difficult for us to integrate new knowledge. For example, even undergraduate students with substantial mathematical background often struggle with understanding new mathematical concepts. They may mistakenly assume the converse of a theorem, or get caught up in concrete ways of thinking about abstract concepts \citep{Hazzan1999}. Even if they can learn a basic concept rapidly, it may be hard for them to apply it in more abstract situations or to extract more formal understanding from it \citep[e.g.]{Lampinen2017b}. Indeed, even students who complete a course in geometry in high school may not achieve formal deductive understanding of the concepts taught unless they become undergraduate mathematics majors \citep{Burger1986}. Indeed, superficial details of how a concept is presented can have profound impacts on how easy it is to reason about, even if the underlying concept is exactly the same \citep[e.g.]{Kotovsky1985}. \par 
Similarly, we cannot always flexibly use the knowledge we have. For example, even mathematics students who can correctly state a rule or theorem are not necessarily able to apply it to create a proof \citep{Weber2001}. Likewise, rapid transfer of knowledge is often only possible when superficial details match closely, and so \citet{Detterman1993} has argued that siince inducing transfer often requires manipulations ``with the subtlety of a baseball bat,'' we can conclude that ``significant transfer is probably rare and accounts for very little human behavior.'' This is a particularly tendentious presentation of the issues, but it captures a broader insight that humans are not always rapid learners or flexible reasoners. \par 
How can we reconcile the demonstrations of rapid learning and flexibility with the evidence that some concepts are learned slowly and some knowledge is inflexible? There a variety of factors. We need high-quality representations of the concepts we are learning in order to reason flexibly with them. These generalizable representations are generally created through making connections between different pieces of our knowledge \citep{Wilensky1991, Schwartz2015}. Relatedly, we often need strategic meta-knowledge about where and how to apply our knowledge in new situations, which also must be learned \citep{Weber2001}. Both these factors mean that transfer may happen more easily over longer periods of time, as I have argued in my prior work \citep{Lampinen2017}. The quality of the representations we have, and the way those representations relate to the new tasks we are presented with, affects our ability to learn rapidly and reason flexibly. \par 

\subsection{What factors contribute to our flexibility?}
With the above caveats in mind, we need to address the computational question of how we are able to learn rapidly and flexibly. There are a variety of factors and systems that contribute to human flexibility and rapid learning. In this section I will review a few of these. \par 

\textbf{Complementary learning systems:} First, it has been proposed that we have complementary learning systems \citep{McClelland1995, Kumaran2016}, which allow us to learn rapidly from new knowledge while avoiding catastrophic interference \citep{McCloskey1989} with the statistical knowledge we have accumulated over longer timescales. That is, the slow learning system sets up good representations, while the fast learning system stores new knowledge by using these representations. Throughout this paper, we will return to this theme of fast and slow learning which support each other. I will therefore divide the rest of this section into a ``slow'' and a ``fast'' part. \par
\subsubsection{Slow}
\textbf{Culture:} One critical example of this is culture. Our cultures have accumulated knowledge over extremely long time scales that allows us to advance much more rapidly now. Before they graduate high school, many children in the US gain fluency in concepts like calculus that took thousands of years to discover. Because culture has set up useful representations for these concepts, we are able to acquire them much more rapidly \citep{McClelland2016}. \par 
\textbf{Transfer between tasks:} Even if culture has not explicitly highlighted (or engineered) structural relationships between tasks, we can benefit from structural similarity. For example, after learning an artificial grammar, subjects can generalize their knowledge to novel sequences from the same grammar applied to novel symbols \citep[e.g.]{Tunney2001}. From learning about simple harmonic oscillators in the context of springs, participants can transfer to a superficially unrelated problem about controlling the population of a city \citep[e.g.]{Day2011}. Because many tasks we perform share deep underlying structures, we can take advantage of transfer to learn faster on new tasks. \par 
\textbf{Grounding and representation quality:} One particular type of transfer that seems to be especially useful is grounding \citep{Barsalou2007}. In particular, conceptual representations often tend to be tied into more basic perceptual-motor systems, e.g. arithmetic in the Approximate Number System \citep{Park2013}, or mathematical concepts in gestures \citep{Goldin-Meadow1993}. Indeed, even our perception of symbols seems to be influenced by ``meaningless'' perceptual details like their spacing \citep{Landy2007}. Because our perceptual-motor system have exceptionally good representations that are trained over long developmental time-scales, we may benefit from leveraging these representations to transfer our understanding to analogous conceptual domains. \par 

\subsubsection{Fast learning}
\textbf{Hippocampal:} From the complementary learning systems perspective, the hippocampus serves as a fast learning system which can store an essentially unlimited number of distinct experiences while minimizing interference, i.e. as a nonparametric learning sytem \citep{Kumaran2016}. This makes it an excellent sub-system for learning from a small amount of data, because it can store a few experiences and allow them to be retrieved at a later time. It can also help with integrating this knowledge without catastrophically interfering \citep{McCloskey1989} with knowledge gleaned from prior experiences, by allowing interleaving of these prior experiences in learning. This can potentially occur in a usefully biased way \citep{Kumaran2016}. There may even be interesting computations performed within the hippocampus to support generalization \citep{Kumaran2012}. \par 
\textbf{Interactive learning \& hypothesis testing:} Humans are able to use our stored experiences to rapidly learn, by behaving as though we are formulating and testing hypotheses, even from a very young age \citep{Sobel2004, Gopnik2014}. We can even take advantage of these hypotheses in order to actively learn and acquire information from the world that is most useful for us \citep[e.g.]{Markant2014a}. By using our prior knowledge of the world to help interpret new experiences, we are able to make extremely fast inferences about how to understand a new situation. \par

\subsubsection{Interactions between fast and slow learning systems}
At their core, most of these issues revolve around a synergy between learning across different timescales (as highlighted in complementary learning systems). In particular, the knowledge that our cultures have accumulated over millenia and that we have accumulated over our lifetimes allows us to constrain the hypothesis space for new learning, so that we can successfully infer structure from few examples in new situations. \par 
From my perspective, these slowly-learned regularities in the world can occur in multiple places. They can occur in the mapping from inputs to our awareness, for example in visual cortex neurons which adapt to the regularities encountered over development \citep{Barlow1975}. However, they can also occur in the systems that implement higher-level computations. For example, the results on transfer in artificial grammar learning described above \citep[e.g.]{Tunney2001} show that humans are able to transfer knowledge at the level of structures or algorithms. The limits of this transfer are as yet unclear; a part of the work in my dissertation will attempt to explore them. \par  

\textbf{Limitations \& tradeoffs:} Of course, there are trade-offs to relying on transfer and prior knowledge. When new tasks are not well aligned with our prior knowledge, relying on prior knowledge can actually interfere with learning. For example, this is one piece of the argument that we made \citep{Lampinen2017b} to explain the results of \citet{Kaminski2008}. This is an illustration of the broader phenomenon of negative transfer -- interference effects produced by transferring between non-isomorphic domains. \par

%% TODO: expand on negative transfer

\subsection{Steps towards flexibility in deep learning}

A great deal of recent work in machine learning can be seen as attempting to make machine learning systems more flexible. This work typically falls into two categories: learning multiple tasks with the hope that the additional constraints will improve the representations in the model and cause it to generalize better (multi-task learning), and learning to learn tasks (meta learning). I will review some of this literature in this section.

\subsubsection{Multi-task learning}

Multi-task learning is generally related to the ``slow'' learning systems I described in humans. Typically, parameters are partially shared between the two tasks, and these shared parameters are learned over long time-scales. This can be done either in a sequential fashion (where you use one task to pre-train the network for another), or simultaneously (where you learn multiple tasks at the same time or on alternating gradient steps). Auxiliary tasks need not be of the same type as the main task, for example reinforcement learning tasks can be supplemented with auxiliary supervised tasks like temmporal autoencoding \citep[e.g.]{Hermann2017}, or unsupervised tasks can be used to pre-train for supervised ones \citep[e.g.]{Wu2018}. \par
\textbf{Pre-training:} One example of sequential multi-task learning is the extremely common practice of pre-training a network on some canonical task in order to use one of its hidden layers as a feature for some other related task. For example, pre-training on ImageNet \citep{Deng2009} is often seen as a useful general feature extractor for vision tasks \citep{Huh2016}, even for quite different transfer domains \citep[e.g.]{Marmanis2016}. \par    
Pre-training is used on more than just vision tasks, however. In natural language processing (NLP) applications, the representations of words is often pre-trained on co-occurrence prediction tasks \citep[e.g.]{Pennington2014}. In AlphaGo \citep{Silver2016}, the networks were pre-trained to predict expert go-players' moves, and then were tuned from that starting point using reinforcement learning. \par

\textbf{Simultaneous multi-task learning:} However, many multi-task learning approaches train on the tasks simultaneously. For example, simultaneously training a natural language translation system to do image captioning in the target language improves its translation performance \citep{Luong2016}. Even training it to translate between multiple language pairs is beneficial \citep{Dong2015}. This can even lead to zero-shot generalization to translation between language pairs never seen together in training \citep{Johnson2016a}. \par 
In Reinforcement Learning (RL), auxiliary tasks have been suggested as key to overcoming the problem of signal sparsity \citep[e.g.]{LeCun2016}. They have been used this way on a variety of RL tasks, for example in grounded language learning \citep{Hermann2017}. \par
The observation that deep networks will learn representations that represent shared structure in the tasks they perform, and can exploit this shared structure to generalize better is not new. It was observed at least as long ago as \citet{Hinton1986}, and has continued to intrigue researchers since \citep[e.g]{Rogers2008}.  

\textbf{Towards a theoretical understanding of multi-task benefits:}
* \citep{Lampinen2019}

\subsubsection{Meta-learning \& related approaches}

* Memory based parameter adaptation. 

* The differentiable neural computer (DNC), proposed by \citet{Graves2016}, and its precursors \citep[e.g.]{Graves2014} brought some additional flexibility to the type of computations networks were allowd to do by allowing them to read and write from a memory buffer with a variety of mechanisms for attending to this buffer to look up knowledge. Like other meta-learning systems, this allows flexibility to learn and reason from new knowledge rapidly. However, this flexibility remains fundamentally within the computations done for a task, and does not allow the more general flexibility we seek.

* Other memory-based architectures here

* Jacob Andreas' stuff.

* Summary: meta-learning is starting to solve the small-data problem, but general flexibility remains out of reach. 

* However, something something adversarial reprograminng \citep{Elsayed2018} something something chaotic systems and language/human culture just shapes us so that we can exploit that latent flexibility in useful ways.

\subsection{What's still missing?}

* Flexible remapping.

* Language.

* Abstraction, sharing of knowledge across layers of abstraction/flexibly mapping between different *kinds* of knowledge.

\section{Towards more flexible deep learning architectures}

The fundamental insight of meta-learning is that there is a continuum between tasks and data. We attempt to follow this premise to its logical conclusion by implementing architectures which minimize the distinction between data and tasks. At the same time, our architecture exploits different timescales of learning to allow the system to slowly accumulate knowledge about regularities among data or tasks, and rapidly adapt to new data or tasks consistent with this learned structure \par.

In particular, our system learns to map different kinds of knowledge -- data, tasks, language, etc. -- into a common representational space, and meta-learns to compute mappings over this space. This allows a number of advantages in the ability of the system to rapidly and flexibly learn and adapt. Many of the features of human flexibility are represented under a fairly unified computational principle in this framework. For example, following instructions to perform zero-shot on a task is just a mapping from language to a behavioral strategy. Trying to alter behavior on a task, such as switching from trying to win a game to trying to lose, is a mapping from one behavioral strategy to another. Explaining behavior is a mapping from behavioral strategies to language. Our system makes all these mappings in-principle learnable. \par 

One can loosely think of the common representational space as being something like a global workspace of consciousness \citep{Baars2005, Dehaene2017}, and the mappings within it as being the operations that we can consciously perform. Of course, these operations are constrained by the experiences we've had and the mappings we've been required to make, so the space of conscious representation may be set up to make certain types of mappings more easily realizable than others on certain kinds of data. Nevertheless, we are in principle capable of learning an arbitrary mapping between any sets of things of which we are conscious, whereas we are not necessarily capable of learning an arbitrary mapping between features that are unconsciously represented in the brain, such as the activity of a single neuron. The features which the brain learns to consciously represent are precisely those that are useful for the computations we consciously perform. These prior expectations contribute to our flexibility within the broad bounds of learned schemas, but our difficulty with adapting to tasks which are completely unlike those we've encountered before. Our architecture attempts to imitate this combination of inherent flexiblity and learned specialization. \par  

\subsection{Embedded meta-learning architecture}

\subsection{Results}

\subsection{Future directions}

* Karmilov-Smith \& rerepresentation
* Related: Offloading from conscious to unconscious
    - Consciousness as HoT not(?) captured in this model as is?


\section{Towards better understanding of human knowledge transfer}

* Building off implicit and explicit structure-learning work including on artificial grammar learning and transfer \citet{Cleeremans1991, Tunney2001}. 
* More recent work on more complex graph-structure learning starting with work by Anna Schapiro and colleagues \citep{Schapiro2013}, and continuing to work showing that some graph structures are more learnable than others \citep{Kahn2018}.

* Plan: try an experiment similar to the above, learning the same structures as \citet{Kahn2018}, but where one context is learning them as grammars where they must press the correct key, and the other context is learning them in the multi-key-press context like the original authors. Does similarity of structures between these two tasks predict learning speed on the second task?

* If we observe transfer, does this imply that the results of \citet{Kahn2018} could be due to the presence of structures more like this (hierarchical, clustery) in the world rather than any inherent learnability advantages per se?

* The kinds of transfer effects we see tell us something about how fast the meta-learning system adapts (or how it primes, it may be hard to dissociate). 
* C.f. transfer or learning effects observed by \citet{Bourne1970} in binary category learning. 

\section{Discussion}

    

\section{Reading notes/literature to incorporate(?)}
\subsection{Machine learning}
\begin{itemize}
%% Adversarial
\item Adversarial reprogramming of neural networks -- A cool twist on adversarial examples: the authors were able to "hijack" a variety of networks to perform tasks which were somewhat different from the ones they were trained for by creating an input that consisted of a "program-specifying" perturbation and the data for the new task, and then learning a mapping from the hijacked network's outputs to the new outputs. This is a much stronger (and weirder) kind of transfer than many others, suggesting that the networks are performing deeply non-linear computations and that changes in the inputs can richly explore the space of functions that the networks can compute (i.e. suggesting that the networks are quite chaotic systems). The particular examples they chose share a few more features than I would like (i.e. they are all visual tasks), but nevertheless the results are pretty cool. \citep{Elsayed2018}
\item Adversarial examples that fool both computer vision and time-limited humans -- Shows large-perturbation adversarial examples to humans, and under time-pressure they are more likely to make the same mistakes as the networks. More precisely, the adversarial perturbations systematically reduce performance, whereas a simple change of the adversarial permutation (flipping it vertically) leaves performance mostly unaltered. Furthermore, if they are given a 2AFC where the true class is \textbf{not present} they are systematically biased toward the adversarially targeted class. Under no time pressure, humans accurately classify the images.
%% Transfer & non-RL meta-learning 
\item Label efficient learning of transferable representations across domains and tasks -- Throws together a bunch of tricks for the goal of doing transfer learning with a small amount of data, but the key one is using an adversarial discriminator across the hidden layers of the networks performing the different tasks. Aligning the representations in this way appears to substantially improve transfer. (It's probably crucial here that the transfer network is initialized with the parameters of the source network.) \citep{Luo2017} 
\item An overview of multi-task learning in deep neural networks -- 2017 review of the multi-task learning literature in both simpler machine learning models like SVMs and in deep nets, arguing that soft parameter sharing methods of various kinds are underused compared to ``hard'' parameter sharing as with multi-head outputs, e.g.\citep{Ruder2017} 
\item Identifying beneficial task relations for multi-task learning in deep neural networks -- For 10 NLP tasks, this paper trains both single task models and all task pairings, and then exams the extent of transfer for them, and its predictability. They make the claim that ``multi-task gains are more likely for target tasks that quickly plateua with non-plateauing auxiliary tasks,'' with the suggestion that this helps them keep from getting stuck in local minima (saddle-points?). Obviously the complexity of the target task (here number of labels) is also relevant, but also the entropy of the labels in the auxiliary task is positively associated with transfer, i.e. more uniform auxiliary tasks are more likely to yield transfer (less biasing? just more information?). They say something suggestive about being surprised thatt Jensen-Shannon divergences were not particularly predictive features, but they computed them not between tasks as I would have expected but within-task between word train and val frequency distributions.  \citep{Bingel2017}
\item When is multitask learning effective? Semantic sequence prediction under varying data conditions -- Tries frequency and morpho-syntactic auxiliary tasks for semantic NLP tasks, finds that benefits come mostly from auxiliary tasks with \emph{high entropy} and \emph{low skewness}, and that most main tasks don't benefit much (because semantics and syntax are sufficiently separable?). Note that the tasks that do benefit are often the most complex ones and/or ones like question answering where syntax could be highly relevant. \citep{Alonso2017} 
\item Few-shot autoregressive density estimation -- Uses an autoregressive (PixelCNN) approach with attention over source images to meta-learn generalization on Omniglot like tasks (i.e. see a few samples from a distribution, try to generate more from it). This is like GANs, but with an overarching prior distribution that is slowly learned, while you are rapidly generalizing over conditional sub-distributions. Nice work, but more or less the usual meta-learning trick applied to some different visual architectures. \citep{Reed2018}
\item Meta learner with linear nulling -- An approach to meta-learning (demonstrated on few-shot visual recognition) that intuitively involves doing a on-the-fly computed projection of the embedding vectors for each task. More specifically, the embedding is computed such that every example's embedding is closer (cosine similarity) to the weights in its category than to the weights that decode other exemplars, via a null-space projection on an operator defined by the differences between these. Essentially, this is trying to separate the examples along dimensions where the weights are \emph{null to within category variations}. It's a neat little trick, and seems to improve performance on complex tasks. It seems like there might be simpler ones that would work too, I wonder if they have been tried. \citep{Yoon2018}
\item Neural task programming: learning to generalize across hierarchical tasks -- A neural programmer-interpreter derivative that does learning of hierarchical task structure from demonstrations, using a few clever extensions including learned demonstration subsequence selection for sub-tasks, and getting rid of LSTMs in the controller to avoid getting ``stuck'' in incorrect action trajectories if the environment doesn't respond exactly as expected.
\item Probabilistic model-agnostic meta-learning -- An extension of MAML that involves 1) adding noise to the fast-adaption gradients and 2) doing variational inference in order to make this sample from a posterior over possible models. They show that this gives kinda reasonable samples from posteriors over models of some toy (and one non-toy) datasets.  
\item Meta Networks -- Fairly vanilla meta learning but with both fast (i.e. hyper-network-specified) and slow (trained by gradient descent) weights in both the meta learner and task learner, and with the meta learner seeing gradient info from the task learner. Equals human performance on omniglot.
\item Meta-learning for semi-supervised one-shot classification -- Does this obvious thing(s) for integrating unlabeled examples into few-shot classification learning: try to estimate based on the labeled examples what each classes distribution is, and then softly include the unlabeled examples according to their likelihood under those distributions. More generally, have a neural net learn how to figure out whether to include examples based on various statistics about the distributions.
\item Zero-data learning of new tasks -- Learning tasks from purely linguistic (or other symbolic) descriptions. They consider both just giving a task description as input like any other and conditioning the model on it directly (in the standard sort of meta-learning way, with a Bayesian discriminator), in their most interesting model by taking a learned feature-processor and conditioning the output weights on the description. It works decently, as one would expect, on the few relatively-simple tasks they tried. 
\item Image to image translation for domain adaption -- Uses consistency along a bunch of different paths (cycles and beyond, so related to consistency things I've thought about) to do domain adaptation of images for training, e.g. from video game driving scenes to real street scenes. Really just applies a bunch of different things that had been done previously with tuned weights, but gets pretty good results.
\item PathNet: Evolution Channels Gradient Descent in Super Neural Networks -- Uses network of modules (smaller nets) to learn task, evolves which modules to uses then fixes the most useful ones. By reinitializing other modules, and learning from this starting point, they are able to achieve transfer benefits on a variety of tasks. \citep{Fernando2017}
\item Learning to Make Analogies by Contrasting Abstract Relational Structure -- Does a simple Raven's Progressive Matrices analog where a network sees a ``source'' sequence that obeys some relationship, and then sees a ``target'' sequence, and has to figure out what the last element of the target sequence should be from a set of candidates. They show that if the false candidates contrast different possible relationships (as opposed to randomly sampled), the network seems to learn abstract features and generalize better. Obviously relates to curriculum and active sampling issues. \citep{Hill2019}
%% RL
\item A deep hierarchical approach to lifelong learning in minecraft -- Does mostly obvious tricks to incorporate options (in the RL sense) into DQN-style learning -- learns options as (possibly distilled) indepndent subnetworks, learns a policy for choosing whether to use a primitive action or an option, and then follows options until a (learned) termination criterion. For all that fancy architecture, it still doesn't have either the flexibility to compose skills or learn more than a fixed number of skills, and the results are far from astonishing. \citep{Tessler2016}.
\item RL$^2$: Fast reinforcement learning via slow reinforcement learning -- The standard deep-learning meta-learning tricks applied to RL. Instead of running an RL algorithm on a single task, train an RL algorithm such that it does well at choosing the next action on *any* given MDP from the sequence of previous states it has seen in that MDP, much as a human might learn to perform quite well on a videogame based on their previous experiences in it. The network does this in the most obvious way: just slap some recurrence in your policy network which maps from (S, A, R, S) tuples to actions and let gradient descent sort the rest out. Works pretty well on some toy non-toy tasks (relatively simple tasks dressed up with high-dimensional input and action spaces).
\item Learning to reinforcement learn -- Much like the RL$^2$ paper above, slaps some recurrence in a network which maps from (S, A, R) tuples to policy and value estimates, and then trains it to do well across episodes drawn from a distribution of tasks, then shows that it generalizes and can even show ``model-like'' behavior, in the sense that it will one-shot learn to take actions which lead to more rewarding states, that is, it seems to have learned to integrate observed transitions and rewards into its decisions on the fly. 
\item Guided cost learning: deep inverse optimal control via policy optimization -- One type of efficient learning that humans can do is learning from demonstrations. The problem of inferring an objective function from a demonstration is know as the inverse RL problem in the RL literature. This paper presents an algorithm for this based on inferring the objective and policy at the same time (instantiating at least the objective as a deep net) and demonstrates the success of the algorithm in robotics. \citep{Finn2016}
\item Meta learning shared hierarchies -- A kinda hacky (but intuitively reasonable) approach to shared-option meta-learning where \(k\) different option policies are learned, and then on each task a (task-specific) master policy is learned which just chooses between them. On a new task, they burn-in updates of the master policy to get semi-reasonable performance, and then do alternate updating of the option policies and the master policy. It does pretty reasonably well when you can assume that all your options should be shareable between tasks, but it's not clear how applicable it would be to domains where this is not true. It also doesn't pass any gradients between the master and option policies which (as they themselves admit) probably makes learning complex things more difficult. Also only one level of hierarchy, as usual. \citep{Frans2018} 
\item Some considerations on learning to explore via meta-reinforcement learning -- Proposes to make meta-RL systems do better at exploring by explicitly accounting for it in the loss. Gives modifications of MAML and $RL^2$. For MAML they add an additional loss term which rewards actions in an initial (hard-coded) exploratory phase which increase the rewards later (they don't see much improvement from this, and my guess is that it's because it's really hard to estimate this loss term). For $RL^2$ they just zero out the loss from the initial exploratory period, which seems to work pretty well. Both methods do better than (either) baseline in general. Interestingly their modification of $RL^2$ is generally the best asymptotically (in meta training) even though the $RL^2$ baseline is much worse than either MAML or their modification of it.
\item Life-Long Disentangled Representation Learning with Cross-Domain Latent Homologies -- Essentially an extension of a VAE that classifies inputs as coming from different environments and tries to learn (partially overlapping) latent dimensions which explain those data, doing some masking of dimensions that seem irrelevant for the present example so they don't get biased, and using replay (hallucination based on old parameters rather than actually keeping memories) to prevent catastrophic interference. \citep{Achille2018}
\item Neural Episodic Control -- Nonparametric DQN, essentially. Instead of computing Q-values parametrically from state representation, stores memories of previous state representations and Q-values, and then uses those to produce similarity-weighted Q-value estimates for new states. This learns fatster (and often does better) than parametric models. \citep{Pritzel2017}
%% NLP or "symbolic" architectures
\item Relational inductive biases, deep learning, and graph networks -- Argues that we should build networks that have graph-structured inductive biases about the world, given that the world has relational structure and (insert Gentner here) says it's super important for our reasoning. They propose a general type of networks like this which update as follows: each edge is updated depending on its state, its endpoints states, and the graph state;, each vertex is updated depending on its state, its edges' states, and the graph state; and finally the graph state is updated depending on its state and all its vertices' states. Relates this to an number of other graph/set networks that have been proposed recently. They suggest composing these graph-network blocks much like one would compose layers in a standard neural net, including the possibility of things like recurrence. (However, the constraint of ``things must be the same size'' to be combined in a standard net scales to the much more limiting constraint of ``things must be the same graph'' in this architecture -- this may not be devastating, but isn't there a better way?) They argue that networks built on these principles, because they reuse computations in a way that respects graph structures, will generalize more robustly to new situations. They cite a number of papers that prove this in the case of generalizing to e.g. larger graphs (``combinatorial generalization''). However, they point out that their invariance to node permutations makes certain kind of graph computations difficult, and furthermore that graphs do not make ideas like recursion easy to represent. Furthermore, the type of combinatorial generalization they show is still far from the flexibility of human intelligence.\citep{Battaglia2018} 
\item DARTS: Differentiable Architecture Search -- It's not actually full architecture search that is differentiable, but more searching for blocks (like Inception blocks). The basic idea is very simple: just use a softmax over all possible operations (e.g. conv, pooling, etc.). This is then differentiable, but they are being tricky and optimizing the architecture with respect to the validation loss after a single step of gradient descent on the weights using the train data. Because of this, they find that first order optimization of the architecture alone is insuffficient, so they approximate the second derivative with a finite difference weighted by a tunable parameter, and it works well enough for their purposes. An interesting step towards differentiable architecture search, and they show it speeds up learning certain kinds of cells quite a bit, but the title oversells it a bit I think.  
\item Tensor Product Generation Networks for Deep NLP Modelling -- Implements a NLP architecture that allows for (but does not necessitate) tensor-product-representation-like ``unbinding'' of fillers from roles, achieves state of the art on MSCOCO captioning with it, and then shows that the unbinding vectors are actually fulfilling at least some grammatical functions (although there is probably more mixed in there, and they certainly don't show that they're handling all the grammatical functions, just that they mostly cluster nicely by certain grammatical features).
\item Symbolic functions from neural computation -- Starts from the basic ideas of tensor product representations and symbols as encoded as patterns of activation in an (infinite-dimensional) vector space, and shows some surprisingly complex computations, such as recursive access to node information or children in binary trees, or various grammatical rules, can be reduced to \emph{linear} transformations within these vector spaces.  
\item Optimization and quantization in gradient symbol systems: a framework for integrating the continuous and discrete in cognition -- Essentially a more readable follow-up to \citep{Smolensky2006} above, which goes on to argue that the way to have discrete output attractors from a continuous system is to have essentially seperate \emph{optimization} (continuous, via SGD) and \emph{quantization} (soft discretization), and linearly interpolate from the former to the latter over processing time. Argues that this makes good quantitative predictions about speed accuracy tradeoffs, e.g. \citep{Smolensky2014}
\item Learning and analyzing vector encoding of symbolic representations -- A short paper reviewing some methods of symbol binding in neural nets and proposing to just throw LSTMs at the problem instead, and then showing a very weak demonstration that it works (the embedding vectors kinda obey the basic superposition rule you would want them to). \citep{Fernandez2018} 
\item Improved semantic representations from tree-structured long short-term memory networks -- Points out that there's no reason for the simple chain-style graphs of typical recurrent networks, any DAG will do. In particular, they build tree-structured networks on the fly from various parse trees (constituency and dependency) of sentences, and show that processing the inputs through these networks yields better performance than basic LSTMs on a sentiment and semantic tasks. \citep{Tai2015} 
\item Ask me anything: dynamic memory networks for natural language processing -- Proposes a fairly complicated memory based architecture for NLP tasks like bAbI. Basically, the model consists of a probe encoder, the probe embedding is then used as attention over the input to generate ``episodic'' encodings of each input chunk (sentence or word depending on the task) of the input. Then the memory network does a fixed number of passes of episodic ``recall'' conditioned on 1) the question, and 2) the previously recalled memory, via a GRU with (atypical) attention based on the episode, the previous memory, and the question. The final memory produced is passed as input to the answer module, along with the question. This architecture slightly outperforms simpler architectures at most tasks (including the paper above at sentiment analysis). \citep{Kumar2015}
\item Learning with Latent Language -- Pre-learns what is essentially a simple domain-dependent image-captioning model, and then shows that inferring a latent language description that optimally performs on the task (or better yet, sampling many and taking an average) works better than other meta-learning strategies, even multi-task ones that have language learning during pre-training. But is it because of their uniquely symbol-structured tasks? The RL task doesn't use pixels as input, and they provide direct policy supervision during the pre-training part. \par 
%% Other
\item Insights on representational similarity in neural networks with canonical correlation -- Proposes a simple generalization of CCA based on accounting for the variance explained by each component as well as the correlation. Shows that this captures some interesting features of relationships between neural networks. First, networks that are ``memorizing'' are less similar under this metric than networks that generalize better. Furthermore, even for networks that generalize well, more similar hyperparameters result in more similar representations at end of training, i.e. there is a ``signature'' of the hyperparameter choices made left behind in the representations. They also observe ``bottom-up'' convergence, where the lower layers of networks (including RNNs) get more similar before the later layers do.  
\item On the importance of single directions for generalization -- Ablation/noise robustness of neural networks is related to generalization, networks that memorize more use more ``single directions'' of representation space to encode dimensions. They find that dropout only increases ablation robustness up to the dropout threshold, whereas batch norm substantially increases it. They also find, interestingly, that batch norm substantially decreases class selectivity of any given filter in a CNN, which they suggest explains the previous result. The overall single directions result seems easily explained from Surya's and my generalization theory paper. \citep{Morcos2018}
\item Outrageously large neural networks: the sparsely gated mixture-of-experts layer -- proposes a neural net ``layer'' that consists of many smaller ``expert'' networks (single hidden layer in their examples) that are sparsely selected via importance weights from a top-$k$ selection over a (noisy) softmax based on a learnable linear transform of the input. The model loss includes a term for the coefficient of variation of the importance weights, encouraging many experts to be used (otherwise there is a rich-get-richer problem where the most selected experts become selected even more). Stacking this layer in between LSTMs improves performance on language modeling and translation tasks. \citep{Shazeer2017} 
\item Deep visual-semantic alignments for generating image descriptions -- A paper on image captioning that was one of the first to propose the idea of conditioning RNNs on CNN reps to caption images, and also contains some interesting hints of consistency-based auxiliary tasks based on aligning saliency selected sub-regions of an image (selected for containing objects) with subsets of the caption. \citep{Karpathy2017} 
\item Unsupervised Feature Learning via Non-Parametric Instance Discrimination -- Learns surprisingly good visual features in an unsupervised way by trying to do classification of each instance as separate from others. These features generalize well to classification (in a non-parametric way), but also do better than supervision-generated features at transfer to unseen datasets. \citep{Wu2018}

\end{itemize}

\subsection{Cognitive Science}
\begin{itemize}
\item Analogy and abstraction -- A review of analogical reasoning in learning, with a slightly devo focus, exploring phenomena such as superficial similarity driving alignment of domains, how this impacts transfer and when people are able to make more abstract alignments. My (heavily biased) spin on their conclusions is that there is a progression from relations per se to relations-as-objects (cf. Annette Karmilov-Smith) and the claim is that then more abstract relations-among-relations are really just relations among objects which are results of the previous process. \citep{Gentner2017}
\item Similarity and the development of rules -- A review of some of the analogical reasoning literature arguing that similarity (even superficial similarity) is important in abstract reasoning, for example children can learn an abstract relationship more easily if they see superficially similar examples of it first to scaffold it, and the patterns of interference in analogicial reasoning are strongly suggestive of similarity at various levels of abstraction playing a role in reasoning, for both adults and children.
\item A model of learning by incremental analogical reasoning and debugging -- Argues for a process of as-needed structure mapping by directly choosing known structures and doing a top-down matching process that is sensitive to the structural links. It then locally debugs to ``fix'' over-interpretation of the analogy. \citep{Burnstein1983}
\item Learning overhypotheses with hierarchical Bayesian models -- Makes the point that hierarchical Bayesian models can capture learning at different levels of abstraction, acknowledges that the highest level prior must still be built in, but makes the tendentious claim that it can be made simple enough to either be obvious or innate. \citep{Kemp2007}
\item A taxonomy for far transfer -- A review that breaks down transfer studies along axes of content and context (with sub-axes for each) and argues that the disagreement in the literature over whether and to what extent transfer occurs is due to the conflation of these many different types of transfer. Their take, while critical, is generally positive on the existence of far transfer. \citep{Barnett2002}
\item All other things being equal: acquisition and transfer of the control of variables strategy -- Explores teaching the control of variables strategy (i.e. holding everything else constant when testing the effect of one variable) to elementary school children, and seeing how well they're able to apply it and transfer it to new problems (the problems were learning about springs, ramps, and sinking in water). In general, older students did better at both applying and transferring, and students who received explicit instruction as well as probe questions (which made them think about the revelant ideas) did better than students who received only probe questions or nothing. The strategy was transferable to problems that were in some ways superficially different (experimenting on different objects), but where the overall context was similar. Many students in all conditions used (and often mentioned) the strategy at least once. However, relatively few students in any condition were able to explicitly identify the strategy when asked about similarities between the problems. \citep{Chen1999}
\item Statistical learning:  from acquiring specific items to forming general rules. -- Short review arguing that rule-like learning in both adults and children is just statistical learning over appropriate features given the input variability. 
\item Generalization, similarity, and Bayesian Inference -- Bayesian models of category formation and generalization can capture aspects of these phenomena that classical models of Shepherd and others cannot; Priors which assume active sampling of positive examples yield effects like narrowing of CIs (cognitively) around observed exemplars as N increases. \citep{Tenenbaum2001}
\item Two modes of transfer in artificial grammar learning -- Different types of transition features seem to be learned by different processes, or at least processes which are dissociable in terms of how noise affects transfer to novel domains. In particular, repetition is a feature that lends itself to creating abstract ``analogy''-like features which can transfer despite noise (where the transition doesn't always occur even if the first item does), but longer distance pairwise dependencies are very sensitive to noise in co-occurence probabilities. \citep{Tunney2001}
\item Getting symbols out of a neural architecture -- Explores the problem of role-filler binding in neural nets, and argues from basic computational principles that vector addition is the right approach (i.e. have a ``role'' vector and a ``filler'' vector and add them), rather than fancier approaches like outer products, because addition has the right compositional structure for human flexibility. \citet{HummelSomeYear}
\item On the proper treatment of connectionism -- BBS article by Smolensky from the early days of connectionism, articulating a position on the relationship between symbolic and sub-symbolic computation and arguing that emergence is an important and useful way of cognitve function even in highly symbolic domains. Takes a somewhat too discrete dual-systems-like view for my taste, but a good source for old quotes about the relationships between symbols and intuitions in cognition and neural networks. \citep{Smolensky1988}
\item The cognizer's innards: a psychological and philosophical perspective on the development of thought -- Argues for Karmiloff-Smith's \emph{representational redescription} (RR) theory; that we successively create more abstract representations by re-representing our prior knowledge. Points out that connectionist networks typically lack the ability to reason flexibly over their own representations, and suggests that overcoming this may require hybrid models, but points out that they will probably need to be more general than the simple one-level hybrid models that include just one mapping from distributed representations to symbols. Some interesting examples of where to look for evidence of redescription in progress. \citep{Clark1993}
\item From meta-processes to conscious access: Evidence from children's metalinguistic and repair data -- A more detailed review of AKS' theory of representational redescription and the evidence for it from patterns of children's linguistic errors and (sometimes unecessary) repairs and a few other nice examples thrown in. In particular, there is a very nice example of children failing to balance an oddly shaped block when asked to do so, but successfully balancing it when asked to build a house, because \emph{their explicit theory is that blocks balance in the middle, but their procedural knowledge is more sophisticated}, but asking them explicitly to do something calls up their conscious knowledge primarily. Also contains the most explicit statement of her ``model'' in any of her work that I've read to date, although still not close to being a quantitative model. \citep{Karmiloff-Smith1986}
\item How the Abstract Becomes Concrete: Irrational Numbers Are Understood Relative to Natural Numbers and Perfect Squares -- Looks at how people understand irrational numbers (actually only square roots), which the authors apparently consider to be a ``highly abstract'' mathematical concept. They find that people do tasks like magnitude estimation with irrationals by relating them to more easily understood numbers. People who have better strategies for this do better. Their only interesting finding to me is a negative: unlike for natural numbers and rationals, better performance on magnitude-type tasks was not related to more conceptual knowledge about irrationals (but number-line estimation was). Useful as it relates to \citep{Wilensky1991}. 
\item Metaphor: Bridging embodiment to abstraction -- Argues that metaphors allow embodied cognition to extend to more abstract concepts. Is this a useful point or a tautology? 
\item Rule abstraction, model-based choice, and cognitive reflection -- Compares subjects use of more low-level and more abstract strategies on two tasks (feature vs. rule-based generalization of categories, model-free vs. model-based RL) and the cognitive reflection task (which requires overriding a superficially obvious response). Positive associations where they would be expected, partially but not completely mediated by cognitive reflection. Some interesting dissociations, e.g. some subjects could give the rule when prompted but didn't perform in accordance with it. Model-based choice appeared to be related to rule-based generalization, but model-free is independent of it (model-based and model-free make predictions about two different test trials in their task). Their measures are correlational; their conclusions are vague. 
\item The role of gesture in communication and thinking -- Gesturing is found broadly, seems to serve important functions for communication, \textbf{and}, she argues, for the speaker's thinking. \citet{Goldin-Meadow1999}
\item Transitions in concept acquisition: using the hand to read the mind -- Gestures-speech mismatch in children problem-solving signaled readiness to learn a new concept that their gestures were hinting at, though they could not yet state it.
\item Organizing conceptual knowledge in humans with a gridlike code -- Trained subjects to locate points within a two-dimensional ``conceptual'' space (bird leg length and bird neck length). They showed subjects morphs of birds at a specific angle in this space and had them do a categorization task on the outcomes. Found that there was a bias towards hexagonal symmetry in their representations, such that the fMRI signal showed a sinuisoidal modulation with a period of 60 degrees. Furthermore, the degree of this bias was correlated with peformance. They suggest that this means that subjects are relying on grid cell codes to represent even this conceptual space. Obvious grounding connections here, but would this result in any effects that are measurable purely behaviorally? Also, why would subjects necessarily have to scale both dimensions with the same unit? The angles of course depend on that... Is it because these are really both spatial dimensions? \citep{Constantinescu2016}\par
\item How, whether, why: Causal judgements as counterfactual contrasts -- Tobi's billiard-ball causality paper. Worth thinking about how intuitively people understand the necessary/sufficient distinction in this context when they struggle with it in others. (Like the Wason selection task social cover story.) \citep{Gerstenberg2015} \par
\item Medial prefrontal cortex predicts internally driven strategy shifts -- Gave people an explicit but somewhat difficult task, where there was a (non-instructed) feature (color) that was actually a deterministic cue to the correct response. Participants who reported having noticed this showed sharp shifts in behavior, but these shifts were \textbf{preceded} by color being decodable from PFC. Interesting w.r.t. the issue of whether and how implicit statistical learning becomes explicit. However, there's a high chance of this time ordering being artifactual with the way they computed when the behavioral shift occurred. Talked to Paul Muhle-Karbe about possibility of running an interrupted behavioral paradigm to try to get a bit more traction. \citep{Schuck2015} \par
\item Learning the structure of event sequences -- Jay \& Axel's paper on implicit learning of grammars \citep{Cleeremans1991}.
\item Network constrainsts on learnability of probabilistic motor sequences -- Paper Danielle Bassett talked about in MBC with how graph structure affects implicit learning a la Schapiro or Cleeremans. \citep{Kahn2018} 

\item Training the approximate number system improves math proficiency -- Causally experiment showing that ANS training with adding and subtracting dot arrays improves scores on symbolic addition and subtraction!! \citep{Park2013}
\item Improving arithmetic performance with number sense training: an investigation of an underlying mechanism -- Digs deeper into the above results, showing that other kinds of targeted training don't help, and doing an \textit{okay} (non-)control  experiment showing that verbal suppression doesn't interfere with the ANS task (but not actually showing that it doesn't interfere with the transfer effect to symbolic). \citep{Park2014}
\end{itemize}


\section{Transfer}
It has been frequently noted that human learning is efficient, \citep[e.g.]{Gentner2003}, and in particular that it is much more data-efficient and flexible than current deep learning systems \citep{Lake2016}. Why might this be? \par
One possible answer is to say that there are architectural and algorithmic differences between humans and neural networks, so deep learning is simply not a good model of human learning. However, there are also a number of differences between the \emph{situations} in which we evaluate human learning and deep learning. One potentially crucial difference is that humans come to tasks with a vast amount of prior experience. Often this prior experience is related in some way to the new tasks we are learning, both through generic structures like language and more specific ones like mathematical or game-playing expertise \citep{Hansen2017}. Thus, humans rarely learn a task from a ``blank slate.'' \par
This is important from the perspective of transfer as ``preparation for future learning'' \citep{Bransford1999}. This perspective suggests that we should think about not just how transfer impacts immediate performance on a new task, but how it impacts the ability to \emph{learn} that task. It is possible that the effects of relevant prior experience will not be apparent immediately on introduction to a new domain, but instead will emerge as the subject comes to understand (either implicitly or explicitly) how that prior knowledge is relevant. \par 
Unfortunately, much of the cognitive work on transfer has ignored this perspective \citep{Bransford1999}. This may have resulted in some of the negative claims about transfer, for example the following quote from \citet{Detterman1993}: 
\begin{quote}
``Significant transfer is probably rare and accounts for very little human behavior. Studies that claim transfer often tell subjects to transfer, or use a `trick' to call the subjects' attention to the similarity of the two problems. Such studies cannot be taken as evidence for transfer. We generally do what we have learned to do and no more.''
\end{quote}
The studies that have demonstrated transfer without using the ``tricks'' that Detterman refers to have generally demonstrated it with quite simple tasks such as grammaticality judgements in toy artificial grammars \citep[e.g.]{Tunney2001} or learning of a simple property of a simple physical system \citep{Day2011}. However, humans learn extremely complicated systems of knowledge that share structure with one another. For example, category theory lays out deep analogies between whole fields of mathematics, which nevertheless could be (and were/are) studied independently. Thus, my first goal for my dissertation is to explore the effects of transfer on learning in tasks which are somewhat closer to the complexity of the tasks that humans actually learn. \par
\section{Different types of knowledge}
Another aspect of human learning is that we often learn many different related tasks within the same domain, either at the same time or sequentially. For example, memorizing multiplication tables and studying number theory are two tasks which in some sense deal with the same domain (integers), but which differ in many ways. How does knowledge interact across these tasks? \par
This can be viewed as a type of transfer which is both nearer (in some superficial details) and yet farther (the tasks are very much non-isomorphic, even if some underlying structure is shared\footnote{The problem of how to quantify to what extent two tasks have shared structure is salient but actually quite difficult. For example, consider the functions \(f: \Ints^2 \rightarrow \Ints^2\) and \(g: \Reals \rightarrow \Reals\) such that \(f((x,y)) = (y, x)\) and \(g(z) = 1/z\). These two functions may seem quite different, they have different domains and ranges, etc. However, if you interpret \((x,y) \in \Ints^2\) as \(x/y \in \Rats\), then it becomes clear that these are actually the same abstract function of multiplicative inversion. The relationship between these functions can be expressed by a commutative diagram, but this requires identifying the mappings between the domains and ranges, which could be quite difficult \emph{a priori}.\par
For an even more difficult example, consider the tasks of a) playing chess, and b) identifying pinned pieces on a chess board. While these tasks clearly share some structure, there is no hope that one of them factors through the other -- at best one might serve as a tiny piece of a factorization of the other (i.e. as a useful but not sufficient feature extractor). How can we decide that two functions are related, and how can we mathematically express this relationship?}) than the type of transfer tasks discussed in the previous section. How does this impact learning? Furthermore, how is this impacted by isomorphic tasks from other domains -- does learning about one category (e.g. sets) make one's reasoning more abstract when learning about another category (e.g. groups)? \par 
A final over-arching type of knowledge we can possess is explicit awareness of a relationship between two tasks. In previous work \citep{Lampinen2017a}, I used neural network simulations to suggest that when performing two analogous tasks, the analogies will naturally be reflected in the representations formed. Thus representational similarities could be used as a heuristic when searching for an explicit analogy. \par
My second goal for my dissertation is to understand how different types of knowledge interact. Ideally, I would like to explore both within domain interactions of different types of knowledge, how within-domain knowledge is impacted by transfer across domains, and how explicit knowledge about relationships across domains emerges. \par 
\section{Sketch of my experimental paradigm \& preliminary results}
\begin{figure}[p]
\centering
\begin{subfigure}{0.33\textwidth}
\includegraphics[width=\textwidth]{figures/hexagon_bi.png}
\caption{Flipping structure}
\end{subfigure}
\hspace{3em}
\begin{subfigure}{0.33\textwidth}
\includegraphics[width=\textwidth]{figures/hexagon_tri.png}
\caption{Cycling structure}
\end{subfigure}
\caption{Graph structures used in our experiment. Red and blue arrows correspond to the two different actions participants can take.}
\label{hexagon_diagrams}
\end{figure}
\begin{figure}[p]
\centering
\begin{subfigure}{0.33\textwidth}
\includegraphics[width=\textwidth]{figures/door_example.png}
\caption{Navigation task}
\end{subfigure}
\hspace{3em}
\begin{subfigure}{0.33\textwidth}
\includegraphics[width=\textwidth]{figures/fractal_example.png}
\caption{Fractal mutation task}
\end{subfigure}
\caption{Superficial tasks used in our experiment}
\label{tasks}
\end{figure}
\begin{figure}[p]
\centering
\begin{subfigure}{0.45\textwidth}
\includegraphics[width=\textwidth]{figures/ns_more_basic_plot_fractal.png}
\caption{Subjects in the isomorphic condition find more efficient paths.}
\label{results_ns}
\end{subfigure}
\hspace{3em}
\begin{subfigure}{0.45\textwidth}
\includegraphics[width=\textwidth]{figures/ds_by_iso.png}
\caption{Subjects in the isomorphic condition may do better at identifying their fractal-task structure.}
\label{results_ds}
\end{subfigure}
\caption{Isomorphisms between the graphs improved performance on the fractal task, and may improve the ability to explicitly identify the graph structure underlying the fractal task.}
\label{results}
\end{figure}
I have run a preliminary experiment in that captures many of these issues; in this section I briefly describe this experiment and a few results. \par
The experiment consists of two superficially dissimilar tasks that participants perform across multiple experimental sessions. Abstractly, both tasks consist of learning to navigate around a directed graph using two different actions which correspond to two different edge types; see Fig. \ref{hexagon_diagrams} for the structures we used. The crucial experimental manipulation is whether the graph structures are the same or different in the two tasks. \par
The superficially-different cover stories we used for these tasks were 1) learning to navigate around a house by going through one of two doors in the rooms of the house, and 2) mutating fractals into one another by using one of two mutagens (see Fig. \ref{tasks}). \par 
We evaluated the number of steps it took participants to solve fractal task trials if they had an isomorphic door task vs. a non-isomorphic one, and found that they took significantly fewer steps if the tasks were isomorphic, see Fig. \ref{results_ns}. Furthermore, having an isomorphic task seemed to potentially increase subjects within-task knowledge, for example increasing the probablility that they could correctly identify the graph structure that their fractal task was based upon, see Fig. \ref{results_ds}. Furthermore, participants in the isomorphic condition were above chance at identifying the mapping between the two tasks. Intriguigingly, explicit realization of the analogy between the tasks did not seem to be sufficient or necessary for the transfer results, although they seemed to be somewhat positively correlated.\par
Overall, isomorphisms between the tasks appear to have effects, but there is a superficial aspect of the graph structures which could explain most of the results we saw here (I will explain further in the meeting). We are about to run a follow-up experiment that should address this concern, as well as a few others; I will describe the changes in more detail during the meeting. 
\section{What's next}
First, I need to review the literature on cognitive transfer and abstraction and multi-task learning more thoroughly, in order to understand better the present state of thinking in the field. Second, there is much more experimental work to be done. We need to verify these results and explore their robustness to changes of graph structure, cover task, etc. I'd also like to explore whether the knowledge transfer is occurring implicitly or explicitly or whether it is different for individual subjects, and whether different degrees of explicitness result in different patterns of transfer. Finally, I'd like to try to model both the procedural transfer of knowledge between tasks and the relationship between procedural and explicit knowledge. \par 

\bibliographystyle{apalike}
\bibliography{arrr}
\end{document}
