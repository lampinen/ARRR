\documentclass[10pt]{article}
\usepackage[fleqn]{amsmath}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{mathrsfs}
\usepackage{graphicx}
\usepackage{float}
\usepackage{subcaption}
\usepackage{relsize}
\usepackage{listings}
\usepackage{tikz}
\usepackage{natbib}
%\usepackage{breqn}
\lstset{language=Python,
    frame=single,
    breaklines=true,
    postbreak=\raisebox{0ex}[0ex][0ex]{\ensuremath{\color{red}\hookrightarrow\space}}
}
%%\usepackage[margin=2cm]{geometry}
%%\floatstyle{boxed}
\restylefloat{figure}
\newcommand{\Prop}{\textbf{Proposition: }}
\newcommand{\Prob}{\textbf{Problem: }}
\newcommand{\Prf}{\textbf{Proof: }}
\newcommand{\Sol}{\textbf{Solution: }}
\newcommand{\grad}{\nabla}
\newcommand{\Nats}{\mathbb{N}}
\newcommand{\Ints}{\mathbb{Z}}
\newcommand{\Rats}{\mathbb{Q}}
\newcommand{\Reals}{\mathbb{R}}
\newcommand{\Comps}{\mathbb{C}}
\newcommand{\Prb}[1]{P\left( #1 \right)}
\newcommand{\PT}[1]{P\left( \text{#1} \right)}
\newcommand{\PCon}[2]{P\left( #1 \mid #2 \right)}
\newcommand{\PConT}[2]{P\left( \text{#1} \mid \text{#2} \right)}
\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator{\tr}{\textbf{tr}}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}
\newcommand{\thus}{\quad\mathlarger{\mathlarger{\mathlarger{\Rightarrow}}}\quad} 
\newcommand{\wght}{\mathbf{w}}
\newcommand{\im}{\text{im }}
\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm,nohead]{geometry}
%%\newcommand\scalemath[2]{\scalebox{#1}{\mbox{\ensuremath{\displaystyle #2}}}}
\setlength\parindent{0pt}
\parskip = \baselineskip
\begin{document} 
\title{Transfer and interactions between different types of knowledge}
\author{Andrew Lampinen}
\date{}
\maketitle
\section{Reading notes/literature to incorporate(?)}
\subsection{Machine learning}
\begin{itemize}
%% Adversarial
\item Adversarial reprogramming of neural networks -- A cool twist on adversarial examples: the authors were able to "hijack" a variety of networks to perform tasks which were somewhat different from the ones they were trained for by creating an input that consisted of a "program-specifying" perturbation and the data for the new task, and then learning a mapping from the hijacked network's outputs to the new outputs. This is a much stronger (and weirder) kind of transfer than many others, suggesting that the networks are performing deeply non-linear computations and that changes in the inputs can richly explore the space of functions that the networks can compute (i.e. suggesting that the networks are quite chaotic systems). The particular examples they chose share a few more features than I would like (i.e. they are all visual tasks), but nevertheless the results are pretty cool. \cite{Elsayed2018}
\item Adversarial examples that fool both computer vision and time-limited humans -- Shows large-perturbation adversarial examples to humans, and under time-pressure they are more likely to make the same mistakes as the networks. More precisely, the adversarial perturbations systematically reduce performance, whereas a simple change of the adversarial permutation (flipping it vertically) leaves performance mostly unaltered. Furthermore, if they are given a 2AFC where the true class is \textbf{not present} they are systematically biased toward the adversarially targeted class. Under no time pressure, humans accurately classify the images.
%% Transfer 
\item Label efficient learning of transferable representations across domains and tasks -- Throws together a bunch of tricks for the goal of doing transfer learning with a small amount of data, but the key one is using an adversarial discriminator across the hidden layers of the networks performing the different tasks. Aligning the representations in this way appears to substantially improve transfer. (It's probably crucial here that the transfer network is initialized with the parameters of the source network.) \cite{Luo2017} 
%% RL
\item A deep hierarchical approach to lifelong learning in minecraft -- Does mostly obvious tricks to incorporate options (in the RL sense) into DQN-style learning -- learns options as (possibly distilled) indepndent subnetworks, learns a policy for choosing whether to use a primitive action or an option, and then follows options until a (learned) termination criterion. For all that fancy architecture, it still doesn't have either the flexibility to compose skills or learn more than a fixed number of skills, and the results are far from astonishing. \cite{Tessler2016}.
\item RL$^2$: Fast reinforcement learning via slow reinforcement learning -- The standard deep-learning meta-learning tricks applied to RL. Instead of running an RL algorithm on a single task, train an RL algorithm such that it does well at choosing the next action on *any* given MDP from the sequence of previous states it has seen in that MDP, much as a human might learn to perform quite well on a videogame based on their previous experiences in it. The network does this in the most obvious way: just slap some recurrence in your policy network which maps from (S, A, R, S) tuples to actions and let gradient descent sort the rest out. Works pretty well on some toy non-toy tasks (relatively simple tasks dressed up with high-dimensional input and action spaces).
\item Learning to reinforcement learn -- Much like the RL$^2$ paper above, slaps some recurrence in a network which maps from (S, A, R) tuples to policy and value estimates, and then trains it to do well across episodes drawn from a distribution of tasks, then shows that it generalizes and can even show ``model-like'' behavior, in the sense that it will one-shot learn to take actions which lead to more rewarding states, that is, it seems to have learned to integrate observed transitions and rewards into its decisions on the fly. 
\item Guided cost learning: deep inverse optimal control via policy optimization -- One type of efficient learning that humans can do is learning from demonstrations. The problem of inferring an objective function from a demonstration is know as the inverse RL problem in the RL literature. This paper presents an algorithm for this based on inferring the objective and policy at the same time (instantiating at least the objective as a deep net) and demonstrates the success of the algorithm in robotics. \citep{Finn2016}
%% NLP or "symbolic" architectures
\item Tensor Product Generation Networks for Deep NLP Modelling -- Implements a NLP architecture that allows for (but does not necessitate) tensor-product-representation-like ``unbinding'' of fillers from roles, achieves state of the art on MSCOCO captioning with it, and then shows that the unbinding vectors are actually fulfilling at least some grammatical functions (although there is probably more mixed in there, and they certainly don't show that they're handling all the grammatical functions, just that they mostly cluster nicely by certain grammatical features).
\item Learning and analyzing vector encoding of symbolic representations -- A short paper reviewing some methods of symbol binding in neural nets and proposing to just throw LSTMs at the problem instead, and then showing a very weak demonstration that it works (the embedding vectors kinda obey the basic superposition rule you would want them to). \cite{Fernandez2018} 
\item Improved semantic representations from tree-structured long short-term memory networks -- Points out that there's no reason for the simple chain-style graphs of typical recurrent networks, any DAG will do. In particular, they build tree-structured networks on the fly from various parse trees (constituency and dependency) of sentences, and show that processing the inputs through these networks yields better performance than basic LSTMs on a sentiment and semantic tasks. \cite{Tai2015} 
\item Ask me anything: dynamic memory networks for natural language processing -- Proposes a fairly complicated memory based architecture for NLP tasks like bAbI. Basically, the model consists of a probe encoder, the probe embedding is then used as attention over the input to generate ``episodic'' encodings of each input chunk (sentence or word depending on the task) of the input. Then the memory network does a fixed number of passes of episodic ``recall'' conditioned on 1) the question, and 2) the previously recalled memory, via a GRU with (atypical) attention based on the episode, the previous memory, and the question. The final memory produced is passed as input to the answer module, along with the question. This architecture slightly outperforms simpler architectures at most tasks (including the paper above at sentiment analysis). \cite{Kumar2015}
\item Outrageously large neural networks: the sparsely gated mixture-of-experts layer -- proposes a neural net ``layer'' that consists of many smaller ``expert'' networks (single hidden layer in their examples) that are sparsely selected via importance weights from a top-$k$ selection over a (noisy) softmax based on a learnable linear transform of the input. The model loss includes a term for the coefficient of variation of the importance weights, encouraging many experts to be used (otherwise there is a rich-get-richer problem where the most selected experts become selected even more). Stacking this layer in between LSTMs improves performance on language modeling and translation tasks. \cite{Shazeer2017} 
\end{itemize}

\subsection{Cognitive Science}
\begin{itemize}
\item Analogy and abstraction -- A review of analogical reasoning in learning, with a slightly devo focus, exploring phenomena such as superficial similarity driving alignment of domains, how this impacts transfer and when people are able to make more abstract alignments. My (heavily biased) spin on their conclusions is that there is a progression from relations per se to relations-as-objects (cf. Annette Karmilov-Smith) and the claim is that then more abstract relations-among-relations are really just relations among objects which are results of the previous process. \citep{Gentner2017}
\item Similarity and the development of rules -- A review of some of the analogical reasoning literature arguing that similarity (even superficial similarity) is important in abstract reasoning, for example children can learn an abstract relationship more easily if they see superficially similar examples of it first to scaffold it, and the patterns of interference in analogicial reasoning are strongly suggestive of similarity at various levels of abstraction playing a role in reasoning, for both adults and children.
\item Learning overhypotheses with hierarchical Bayesian models -- Makes the point that hierarchical Bayesian models can capture learning at different levels of abstraction, acknowledges that the highest level prior must still be built in, but makes the tendentious claim that it can be made simple enough to either be obvious or innate. \citep{Kemp2007}
\item A taxonomy for far transfer -- A review that breaks down transfer studies along axes of content and context (with sub-axes for each) and argues that the disagreement in the literature over whether and to what extent transfer occurs is due to the conflation of these many different types of transfer. Their take, while critical, is generally positive on the existence of far transfer. \citep{Barnett2002}
\item Statistical learning:  from acquiring specific items to forming general rules. -- Short review arguing that rule-like learning in both adults and children is just statistical learning over appropriate features given the input variability. 
\item Generalization, similarity, and Bayesian Inference -- Bayesian models of category formation and generalization can capture aspects of these phenomena that classical models of Shepherd and others cannot; Priors which assume active sampling of positive examples yield effects like narrowing of CIs (cognitively) around observed exemplars as N increases. \citep{Tenenbaum2001}
\item Two modes of transfer in artificial grammar learning -- Different types of transition features seem to be learned by different processes, or at least processes which are dissociable in terms of how noise affects transfer to novel domains. In particular, repetition is a feature that lends itself to creating abstract ``analogy''-like features which can transfer despite noise (where the transition doesn't always occur even if the first item does), but longer distance pairwise dependencies are very sensitive to noise in co-occurence probabilities. \cite{Tunney2001}
\item Getting symbols out of a neural architecture -- Explores the problem of role-filler binding in neural nets, and argues from basic computational principles that vector addition is the right approach (i.e. have a ``role'' vector and a ``filler'' vector and add them), rather than fancier approaches like outer products, because addition has the right compositional structure for human flexibility. \citet{HummelSomeYear}
\item On the proper treatment of connectionism -- BBS article by Smolensky from the early days of connectionism, articulating a position on the relationship between symbolic and sub-symbolic computation and arguing that emergence is an important and useful way of cognitve function even in highly symbolic domains. Takes a somewhat too discrete dual-systems-like view for my taste, but a good source for old quotes about the relationships between symbols and intuitions in cognition and neural networks. \cite{Smolensky1988}
\end{itemize}

\section{Transfer}
It has been frequently noted that human learning is efficient, \citep[e.g.]{Gentner2003}, and in particular that it is much more data-efficient and flexible than current deep learning systems \citep{Lake2016}. Why might this be? \par
One possible answer is to say that there are architectural and algorithmic differences between humans and neural networks, so deep learning is simply not a good model of human learning. However, there are also a number of differences between the \emph{situations} in which we evaluate human learning and deep learning. One potentially crucial difference is that humans come to tasks with a vast amount of prior experience. Often this prior experience is related in some way to the new tasks we are learning, both through generic structures like language and more specific ones like mathematical or game-playing expertise \citep{Hansen2017}. Thus, humans rarely learn a task from a ``blank slate.'' \par
This is important from the perspective of transfer as ``preparation for future learning'' \citep{Bransford1999}. This perspective suggests that we should think about not just how transfer impacts immediate performance on a new task, but how it impacts the ability to \emph{learn} that task. It is possible that the effects of relevant prior experience will not be apparent immediately on introduction to a new domain, but instead will emerge as the subject comes to understand (either implicitly or explicitly) how that prior knowledge is relevant. \par 
Unfortunately, much of the cognitive work on transfer has ignored this perspective \citep{Bransford1999}. This may have resulted in some of the negative claims about transfer, for example the following quote from \citet{Detterman1993}: 
\begin{quote}
``Significant transfer is probably rare and accounts for very little human behavior. Studies that claim transfer often tell subjects to transfer, or use a `trick' to call the subjects' attention to the similarity of the two problems. Such studies cannot be taken as evidence for transfer. We generally do what we have learned to do and no more.''
\end{quote}
The studies that have demonstrated transfer without using the ``tricks'' that Detterman refers to have generally demonstrated it with quite simple tasks such as grammaticality judgements in toy artificial grammars \citep[e.g.]{Tunney2001} or learning of a simple property of a simple physical system \citep{Day2011}. However, humans learn extremely complicated systems of knowledge that share structure with one another. For example, category theory lays out deep analogies between whole fields of mathematics, which nevertheless could be (and were/are) studied independently. Thus, my first goal for my dissertation is to explore the effects of transfer on learning in tasks which are somewhat closer to the complexity of the tasks that humans actually learn. \par
\section{Different types of knowledge}
Another aspect of human learning is that we often learn many different related tasks within the same domain, either at the same time or sequentially. For example, memorizing multiplication tables and studying number theory are two tasks which in some sense deal with the same domain (integers), but which differ in many ways. How does knowledge interact across these tasks? \par
This can be viewed as a type of transfer which is both nearer (in some superficial details) and yet farther (the tasks are very much non-isomorphic, even if some underlying structure is shared\footnote{The problem of how to quantify to what extent two tasks have shared structure is salient but actually quite difficult. For example, consider the functions \(f: \Ints^2 \rightarrow \Ints^2\) and \(g: \Reals \rightarrow \Reals\) such that \(f((x,y)) = (y, x)\) and \(g(z) = 1/z\). These two functions may seem quite different, they have different domains and ranges, etc. However, if you interpret \((x,y) \in \Ints^2\) as \(x/y \in \Rats\), then it becomes clear that these are actually the same abstract function of multiplicative inversion. The relationship between these functions can be expressed by a commutative diagram, but this requires identifying the mappings between the domains and ranges, which could be quite difficult \emph{a priori}.\par
For an even more difficult example, consider the tasks of a) playing chess, and b) identifying pinned pieces on a chess board. While these tasks clearly share some structure, there is no hope that one of them factors through the other -- at best one might serve as a tiny piece of a factorization of the other (i.e. as a useful but not sufficient feature extractor). How can we decide that two functions are related, and how can we mathematically express this relationship?}) than the type of transfer tasks discussed in the previous section. How does this impact learning? Furthermore, how is this impacted by isomorphic tasks from other domains -- does learning about one category (e.g. sets) make one's reasoning more abstract when learning about another category (e.g. groups)? \par 
A final over-arching type of knowledge we can possess is explicit awareness of a relationship between two tasks. In previous work \citep{Lampinen2017a}, I used neural network simulations to suggest that when performing two analogous tasks, the analogies will naturally be reflected in the representations formed. Thus representational similarities could be used as a heuristic when searching for an explicit analogy. \par
My second goal for my dissertation is to understand how different types of knowledge interact. Ideally, I would like to explore both within domain interactions of different types of knowledge, how within-domain knowledge is impacted by transfer across domains, and how explicit knowledge about relationships across domains emerges. \par 
\section{Sketch of my experimental paradigm \& preliminary results}
\begin{figure}[p]
\centering
\begin{subfigure}{0.33\textwidth}
\includegraphics[width=\textwidth]{figures/hexagon_bi.png}
\caption{Flipping structure}
\end{subfigure}
\hspace{3em}
\begin{subfigure}{0.33\textwidth}
\includegraphics[width=\textwidth]{figures/hexagon_tri.png}
\caption{Cycling structure}
\end{subfigure}
\caption{Graph structures used in our experiment. Red and blue arrows correspond to the two different actions participants can take.}
\label{hexagon_diagrams}
\end{figure}
\begin{figure}[p]
\centering
\begin{subfigure}{0.33\textwidth}
\includegraphics[width=\textwidth]{figures/door_example.png}
\caption{Navigation task}
\end{subfigure}
\hspace{3em}
\begin{subfigure}{0.33\textwidth}
\includegraphics[width=\textwidth]{figures/fractal_example.png}
\caption{Fractal mutation task}
\end{subfigure}
\caption{Superficial tasks used in our experiment}
\label{tasks}
\end{figure}
\begin{figure}[p]
\centering
\begin{subfigure}{0.45\textwidth}
\includegraphics[width=\textwidth]{figures/ns_more_basic_plot_fractal.png}
\caption{Subjects in the isomorphic condition find more efficient paths.}
\label{results_ns}
\end{subfigure}
\hspace{3em}
\begin{subfigure}{0.45\textwidth}
\includegraphics[width=\textwidth]{figures/ds_by_iso.png}
\caption{Subjects in the isomorphic condition may do better at identifying their fractal-task structure.}
\label{results_ds}
\end{subfigure}
\caption{Isomorphisms between the graphs improved performance on the fractal task, and may improve the ability to explicitly identify the graph structure underlying the fractal task.}
\label{results}
\end{figure}
I have run a preliminary experiment in that captures many of these issues; in this section I briefly describe this experiment and a few results. \par
The experiment consists of two superficially dissimilar tasks that participants perform across multiple experimental sessions. Abstractly, both tasks consist of learning to navigate around a directed graph using two different actions which correspond to two different edge types; see Fig. \ref{hexagon_diagrams} for the structures we used. The crucial experimental manipulation is whether the graph structures are the same or different in the two tasks. \par
The superficially-different cover stories we used for these tasks were 1) learning to navigate around a house by going through one of two doors in the rooms of the house, and 2) mutating fractals into one another by using one of two mutagens (see Fig. \ref{tasks}). \par 
We evaluated the number of steps it took participants to solve fractal task trials if they had an isomorphic door task vs. a non-isomorphic one, and found that they took significantly fewer steps if the tasks were isomorphic, see Fig. \ref{results_ns}. Furthermore, having an isomorphic task seemed to potentially increase subjects within-task knowledge, for example increasing the probablility that they could correctly identify the graph structure that their fractal task was based upon, see Fig. \ref{results_ds}. Furthermore, participants in the isomorphic condition were above chance at identifying the mapping between the two tasks. Intriguigingly, explicit realization of the analogy between the tasks did not seem to be sufficient or necessary for the transfer results, although they seemed to be somewhat positively correlated.\par
Overall, isomorphisms between the tasks appear to have effects, but there is a superficial aspect of the graph structures which could explain most of the results we saw here (I will explain further in the meeting). We are about to run a follow-up experiment that should address this concern, as well as a few others; I will describe the changes in more detail during the meeting. 
\section{What's next}
First, I need to review the literature on cognitive transfer and abstraction and multi-task learning more thoroughly, in order to understand better the present state of thinking in the field. Second, there is much more experimental work to be done. We need to verify these results and explore their robustness to changes of graph structure, cover task, etc. I'd also like to explore whether the knowledge transfer is occurring implicitly or explicitly or whether it is different for individual subjects, and whether different degrees of explicitness result in different patterns of transfer. Finally, I'd like to try to model both the procedural transfer of knowledge between tasks and the relationship between procedural and explicit knowledge. \par 

\bibliographystyle{apalike}
\bibliography{arrr}
\end{document}
